{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba27347c-264c-4ff6-a225-0cd4b2bb8ddf",
   "metadata": {},
   "source": [
    "# Лабораторная работа №1. Крестики - нолики\n",
    "\n",
    "*Арешин Станислав Олегович М8О-211M-21*\n",
    "\n",
    "\n",
    "**Обучаем агента играть против случайной стратегии**\n",
    "\n",
    "* Выгрыш: 1\n",
    "\n",
    "* Проигрыш: -1\n",
    "\n",
    "* Ничья: 0\n",
    "\n",
    "* Ход в ту же клетку: -2\n",
    "\n",
    "**Поле: одномерный массив длины 9, позиция 0 - верхний левый угол, позиция 8 - нижний правый угол по порядку соответственно**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b68ac89-63bf-4504-bcf5-1b63acf239ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631e1e3e-a8d0-477f-8dd6-45f985c48936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(gym.Env):\n",
    "    '''\n",
    "    action_space: возможные позиции для хода 0-8\n",
    "    observation_space: словарь с двумя бинарными векторами с занятыми позициями для каждого игрока отдельно\n",
    "        1 - занята, 0 - свободна\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_space = gym.spaces.Discrete(3 * 3)\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            'agent': gym.spaces.MultiBinary(3 * 3), \n",
    "            'strategy': gym.spaces.MultiBinary(3 * 3)\n",
    "        })   \n",
    "    \n",
    "    '''\n",
    "    Обнуляем доску - все позиции свободны\n",
    "    '''\n",
    "    def reset(self):\n",
    "        self.state = {'agent': np.zeros(3 * 3), 'strategy': np.zeros(3 * 3)}\n",
    "        return self.state\n",
    "    \n",
    "    '''\n",
    "    Агент ходит первым\n",
    "    Если он пытается сходить в занятую позицию: штраф -2 и заканчиваем ход\n",
    "    В случае выгрыша: поощряем +1, заканчиваем игру \n",
    "    Если ничья: 0, заканчиваем игру \n",
    "    Далее ходит случайная стратегия на любую свободную позицию\n",
    "    Если агент проигрывает после хода противника: штраф -1, заканчиваем игру \n",
    "    Ничья: 0, заканчиваем игру \n",
    "    Игра не закончена: 0, следующий ход\n",
    "    '''\n",
    "    def step(self, action):\n",
    "        self.reward = 0\n",
    "        # проверяем на ход в занятую позицию\n",
    "        if self.state['strategy'][action] == 1 or self.state['agent'][action] == 1:\n",
    "            self.reward  = -2\n",
    "            return self.state, self.reward, self.done, {}\n",
    "        # действие агента     \n",
    "        self.state['agent'][action] = 1 \n",
    "        # проверяем победу агента\n",
    "        self.check_win('agent')\n",
    "        if self.done:\n",
    "            self.reward = 1\n",
    "            return self.state, self.reward, self.done, {}\n",
    "        # проверяем ничью\n",
    "        if ~np.any((self.state['agent'] + self.state['strategy']) == 0):\n",
    "            self.done = True\n",
    "            return self.state, self.reward, self.done, {}\n",
    "        \n",
    "        # действие случайной стратегии\n",
    "        player_action = self.get_player_action()\n",
    "        self.state['strategy'][player_action] = 1\n",
    "        # проверяем победу стратегии\n",
    "        self.check_win('strategy')\n",
    "        if self.done:\n",
    "            self.reward  = -1\n",
    "            return self.state, self.reward, self.done, {}\n",
    "        # проверяем ничью\n",
    "        if ~np.any((self.state['agent'] + self.state['strategy']) == 0):\n",
    "            self.done = True\n",
    "            return self.state, self.reward, self.done, {}\n",
    "        \n",
    "        return self.state, self.reward, self.done, {}\n",
    "    \n",
    "    '''\n",
    "    Действие случайной стратегии, выбираем любую свободную позицию\n",
    "    '''\n",
    "    def get_player_action(self):\n",
    "        return np.random.choice(np.argwhere((self.state['agent'] + self.state['strategy']) == 0).reshape(-1))\n",
    "            \n",
    "    \n",
    "    '''\n",
    "    Проверка победы игрока\n",
    "    '''\n",
    "    def check_win(self, player):\n",
    "        self.done = False\n",
    "        if np.sum(self.state[player][:3]) == 3 \\\n",
    "            or np.sum(self.state[player][3:6]) == 3 \\\n",
    "            or np.sum(self.state[player][6:]) == 3 \\\n",
    "            or self.state[player][0] + self.state[player][3] + self.state[player][6] == 3 \\\n",
    "            or self.state[player][1] + self.state[player][4] + self.state[player][7] == 3 \\\n",
    "            or self.state[player][2] + self.state[player][5] + self.state[player][8] == 3 \\\n",
    "            or self.state[player][0] + self.state[player][4] + self.state[player][8] == 3 \\\n",
    "            or self.state[player][2] + self.state[player][4] + self.state[player][6] == 3:\n",
    "                \n",
    "            self.done = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edbc95f-45ed-42aa-a92b-dffd7eff095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 53.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x27c3c8e2cd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# инициализация среды и обучение алгоритма \n",
    "env = MyEnv()\n",
    "model = stable_baselines3.PPO(\"MultiInputPolicy\", env, verbose=False)\n",
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf68b8c-7ba9-48b2-8a0c-d5eb6e1f0c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': array([0., 0., 0., 0., 1., 0., 0., 0., 0.]), 'strategy': array([0., 0., 1., 0., 0., 0., 0., 0., 0.])} False\n",
      "{'agent': array([0., 0., 0., 0., 1., 0., 1., 0., 0.]), 'strategy': array([0., 0., 1., 0., 0., 1., 0., 0., 0.])} False\n",
      "{'agent': array([0., 0., 0., 0., 1., 0., 1., 1., 0.]), 'strategy': array([0., 0., 1., 1., 0., 1., 0., 0., 0.])} False\n",
      "{'agent': array([0., 0., 0., 0., 1., 0., 1., 1., 1.]), 'strategy': array([0., 0., 1., 1., 0., 1., 0., 0., 0.])} True\n",
      "Win\n",
      "[4, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "# играем\n",
    "\n",
    "obs = env.reset()\n",
    "actions = []\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    actions.append(action)\n",
    "    print(obs, done)\n",
    "    \n",
    "if reward == 1:\n",
    "    print('Win')\n",
    "elif reward == -1: \n",
    "    print('Lose')\n",
    "elif reward == 0:\n",
    "    print('Draw')\n",
    "else:\n",
    "    print('Same step')\n",
    "\n",
    "print(actions)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2566a8-bd76-49da-851e-8f7989b901b9",
   "metadata": {},
   "source": [
    "Если обучить недостаточно, агент может проиграть даже случайной стратегии, либо зациклиться на одном и том же ходе, ничью не встречал. Увеличив время обучения, получаем агента, который всегда обыгрывает случайную стратегию."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
